---
title: "Analysis in `Spark` with `sparklyr`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---

# Learning Objectives

* See more examples of simple data wrangling and visualisation in `sparklyr` including `dbplot`
* Be introduced to the `Spark DataFrame` and `Spark MLlib` native `sparklyr` interfaces
* See an example of text mining in `Spark`
* Understand how to benchmark execution times
* Be able to read data from and write data to Apache `Parquet` format

**Lesson Duration: 90 mins**

As usual, let's load `sparklyr` and connect to our local `Spark` installation

```{r}
library("sparklyr")
sc <- spark_connect(master = "local")
```

We're going to be using our normal `dplyr` syntax for analysis, we'll see `dbplot` in action, and we're going to look again at the `Prestige` dataset from the `car` package that we last used in the lessons on multiple regression.

```{r}
library(tidyverse)
library(dbplot)
library(car)
```

```{r}
prestige <- copy_to(sc, Prestige)
```

# Wrangle

Hopefully you're already starting to get a handle on data wrangling using `sparklyr`. As always, the key rule is to push all heavy computation to `Spark`. Let's get the `mean()` of all the numerical variables in `prestige`: 

```{r}
prestige %>%
  summarise_if(is.numeric, mean)
```

Looking at the `SQL` query generated by the pipe, we find that `mean()` has been converted to the `Spark SQL` equivalent `AVG()`

```{r}
prestige %>%
  summarise_if(is.numeric, mean) %>%
  show_query()
```

Here's another example calculating variable variances:

```{r}
prestige %>%
  summarise_if(is.numeric, var) %>%
  show_query()
```

Now we see `R`'s `var()` function has been converted to `var_samp()`.

We've also seen that normal `dplyr` pipes are completely integrated into `sparklyr`:

```{r}
prestige %>%
  mutate(secondary_educated  = if_else(education > 7, "Yes", "No")) %>%
  group_by(secondary_educated) %>%
  summarise(mean_income = mean(income))
```
This is a more involved pipe than the ones we used previously, so let's look at the generated query: 

```{r}
prestige %>%
  mutate(secondary_educated  = if_else(education > 7, "Yes", "No")) %>%
  group_by(secondary_educated) %>%
  summarise(mean_income = mean(income)) %>%
  show_query()
```

The `dbplyr_<num>` object occuring here is a temporary table holding the result of the subquery. One thing to bear in mind when wrangling with `sparklyr` is that `select()` operations will work as expected, but square bracket subsetting does not:

```{r}
prestige %>%
  select(income, education) %>%
  glimpse()
```

Let's try the same thing with square bracket subsetting (we wrap it in a `tryCatch()` function as we expect the worst):

```{r}
tryCatch(
  {prestige[, c("education", "income")] %>% glimpse()},
  error = print
)
```

Hmm, no joy. Just to prove to you that this is a complication of working with `sparklyr`, here's an attempt to do the same operation with the local `Prestige` dataframe:

```{r}
tryCatch(
  {Prestige[, c("education", "income")] %>% glimpse()},
  error = print
)
```

This time it works!

## Using `Spark` built-in functions inside `dplyr` operations

A nice feature of wrangling with `sparklyr` is that we can pass `Spark` functions **into** `dplyr` verb calls. If `R` doesn't recognise the function, it **passes it through** to `Spark SQL` to see if it recognises it instead. This gives us a lot of freedom to mix `Spark` and `R` functions as required!

Here's an example: in what follows, `percentile()`, `array()` and `explode()` are `Spark` functions (technically `Hive` functions)

```{r}
prestige %>%
  summarise(women_percentile = percentile(women, array(0.25, 0.5, 0.75))) %>%
  mutate(
    women_percentile = explode(women_percentile)
    )
```
i.e. if we order occupations by the percentage of women who work in them, low to high, the first $25\%$ of occupations have $3.59\%$ or lower of women working in them, and so on. When we look at the generated query

```{r}
prestige %>%
  summarise(women_percentile = percentile(women, array(0.25, 0.5, 0.75))) %>%
  mutate(women_percentile = explode(women_percentile)) %>%
  show_query()
```

we see that the `Spark` functions have been passed through unchanged, as `dplyr` didn't recognise them as valid `R` functions. You can find the available `Hive` functions [here](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)

<br>
<blockquote class='task'>
**Task - 2 mins**

Have a go at writing a `mutate()` or `summarise()` call of your own on `prestige`. Bonus points if you find an operation that requires you to explicitly use a `Hive` function!

<details>
<summary>**Possible solutions**</summary>
It's actually pretty difficult to find an operation that `sparklyr` cannot automatically convert for you!

```{r}
prestige %>%
  mutate(round_edu = round(education)) %>%
  show_query()
```
```{r}
prestige %>%
  summarise(max_income = max(income)) %>%
  show_query()
```
</details>
</blockquote>
<br>

# Visualise

We've already covered the idea of pushing the heavy computation required in creating plots to `Spark` where possible. So, for example, this would be the correct way to go about creating the following plot:

```{r}
prestige %>%
  group_by(type) %>%
  summarise(number = n()) %>%
  collect() %>%
  ggplot(aes(x = type, y = number)) +
  geom_col(col = "steelblue", alpha = 0.7)
```
while this would be incorrect, as it potentially results in a large dataset being loaded into the memory of your local machine:

```{r}
prestige %>%
  collect() %>%
  group_by(type) %>%
  summarise(number = n()) %>%
  ggplot(aes(x = type, y = number)) +
  geom_col(col = "steelblue", alpha = 0.7)
```

Sometimes though this process won't be possible. A good example is a scatter plot: each point in such a plot corresponds to a row in your dataset, so how can you reduce such data in size before bringing it from `Spark` to `R`? You could randomly sample the data, but this could potentially be misleading if you are unlucky in your sample.

A better approach is to create a **raster plot** using the `dbplot` package. You can think of a raster plot as being like a **binned** scatter plot, i.e. one in which the $x-$ and $y-$coordinates have been binned. Let's see an example for `prestige`:

```{r}
library(dbplot)

prestige %>%
  dbplot_raster(x = education, y = income, resolution = 20)
```

Each block in the bin corresponds to a little 'piece of area' of the scatter plot, and the colour indicates how many points would fall in that area in a complete scatter plot. So, in this case, with `resolution = 20`, only a maximum of $400$ pairs of $(x, y)$ coordinates and counts are returned from `Spark` to `R`, and the number can be considerably lower than that if only return bins in which at least one point fell are returned. The key point here though is that the binning is being done in `Spark`, rather than in `R`! 

# `sparklyr` native interfaces

Now we'll consider the 'native' interface that `sparklyr` presents to `Spark`. 'Native' here means that these functions call `Java` or `Scala` code directly without transformation to SQL! When we think about these interfaces, it's important to understand that `Spark` is much stricter about variable types than `R`:

* `R` like `Python` is a 'dynamically typed' language, we can do things like this:
```{r}
x <- c(1,2,3,4)
x <- "hi"
```

* `Spark` is 'statically typed': variable types have to be defined ahead of time. In the majority of cases, native `Spark` functions take `DoubleType` inputs: this is Spark's equivalent of an `R` `numeric`.

Here are the `R` data types and the comparable `Spark` types

| `R` type | `Spark` type |
|---|---|
| `numeric` | `DoubleType` |
| `integer` | `IntegerType` |
| `logical` | `BooleanType` |
| `character` | `StringType` |
| `list` | `ArrayType` |

We can see the `Spark` data types of each variable using the `sdf_schema()` function:

```{r}
schema <- sdf_schema(prestige)

schema <- schema %>%
  transpose() %>%
  as_tibble() %>%
  unnest(cols = c(name, type))

schema
```
## `Spark DataFrame` interface

* `Spark DataFrame` functions all start `sdf_`: these are useful methods for `DataFrame` sorting, sampling, partitioning and binding.

### Sorting

Sorting is easily accomplished with the `sdf_sort()` function:

```{r}
prestige %>%
  sdf_sort(columns = "prestige") %>% 
  head(20)
```


### Sampling

Downsampling large data sets can be useful at the planning, investigation and exploration stages of a project where you often don't need to use whole data set. It's typical to store downsampled datasets as intermediates in `Spark` using `compute()`. Here we use `sdf_sample()` to downsample:

```{r}
downsampled_prestige <- prestige %>%
  sdf_sample(fraction = 0.2, replacement = FALSE, seed = 42) %>%
  compute("downsampled_prestige")

downsampled_prestige
```

### Partitioning

In 'train-test' or 'train-test-validate' splitting, we want to be able to partition a data set; we use the `sdf_random_split()` function for this:

```{r}
partitioned <- prestige %>%
  sdf_random_split(training = 0.7, testing = 0.3)

training <- partitioned$training %>%
  compute("training")

testing <- partitioned$testing %>%
  compute("testing")

training %>%
  count()

testing %>%
  count()
```

Just as sanity check, how many rows are there in `prestige`?

```{r}
prestige %>%
  count()
```

### Binding

The `sdf_bind_rows()` and `sdf_bind_cols()` functions can be used for binding, exactly as for the `tidyverse` `bind_rows()` and `bind_cols()` functions:

```{r}
reassembled <- training %>%
  sdf_bind_rows(testing)

reassembled %>% 
  count()
```

## `MLlib` machine learning interface

There are two collections of functions in this interface. We have:

* feature transformation functions starting `ft_`: these basically perform feature engineering
* machine learning functions starting `ml_`

Let's see all of the available `ft_` functions:

```{r}
ls("package:sparklyr", pattern = "^ft")
```

And now let's do the same for `ml_` functions:

```{r}
ls("package:sparklyr", pattern = "^ml")
```

As you can see, the provision of functions is very rich! We will examine modeling in more detail in the next lesson, so for now we will limit ourself to looking at feature transformers.

### `ft_binarizer()`

`ft_binarizer()` takes a continuous variable and splits it into binary outcomes by comparison of each value with a threshold: 

```{r}
high_prestige <- prestige %>%
  select(prestige) %>%
  ft_binarizer("prestige", "has_high_prestige", threshold = 65) %>%
  group_by(has_high_prestige) %>%
  mutate(has_high_prestige = as.logical(has_high_prestige)) %>%
  summarise(number = n()) %>%
  collect()

high_prestige %>%
  ggplot(aes(x = has_high_prestige, y = number)) +
  geom_col()
```

### `ft_bucketizer()`

`ft_bucketizer()` is a more general version of the binarizer for binning continuous variables, it is comparable to the base `R` `cut()` function, with a few differences:

* in `cut()` the default is to include the upper and exclude the lower boundary from each bin (`right = TRUE`). In `ft_bucketizer()` the default is to include the lower and exclude the upper boundary
* `ft_bucketizer()` also by default includes upper and lower boundary values in the topmost bin. This is equivalent to using `cut()` with argument `include.lowest = TRUE` (assuming that we have also set `right = TRUE`)

Let's see an example using `cut()` on the original `Prestige` dataframe. We are going to bin the `women` variable: 

```{r}
labels <- c("0 to 30", "30 to 60", "60 to 100")
Prestige %>%
  mutate(
    women_buckets = cut(
      women, 
      breaks = c(0, 30, 60, 100), 
      labels = labels,
      right = FALSE,
      include.lowest = TRUE
    )
  )
```

Let's see what `ft_bucketizer()` returns: 

```{r}
prestige %>%
  ft_bucketizer("women", "women_buckets", splits = c(0, 30, 60, 100)) %>%
  distinct(women_buckets) %>%
  collect()
```

It returns buckets labelled 0, 1 and 2. We should mutate these to meaningful names, but we should **do the `count()` operation on the Spark side**, as this could be an intensive computation 

```{r}
prestige_women_buckets <- prestige %>%
  ft_bucketizer("women", "women_buckets", splits = c(0, 30, 60, 100)) %>%
  count(women_buckets) %>%
  collect() %>%
  mutate(women_buckets = factor(women_buckets, labels = labels))

prestige_women_buckets
```

We can also perform quantile-based bucketing using the `ft_quantile_discretizer()` function:

```{r}
prestige_women_buckets <- prestige %>%
  ft_quantile_discretizer("women", "women_buckets", num_buckets = 3) %>%
  count(women_buckets) %>%
  collect() %>%
  mutate(women_buckets = factor(women_buckets, labels = c("low", "medium", "high")))

prestige_women_buckets
```

# Text data

Let's turn our attention now to text data. We're going to work with the first few paragraphs of 'The Lord of the Rings' by J. R. R. Tolkien and 'The Lion, the Witch and the Wardrobe' by C. S. Lewis.

We can read the text files into `Spark` using the `spark_read_text()` function:

```{r}
lotr <- spark_read_text(sc, name = "lotr", path = "data/lotr.txt" )
lww <- spark_read_text(sc, name = "lww", path = "data/lww.txt")
```

Next we use `sdf_bind_rows()` to bind the `DataFrame`s together

```{r}
all_text <- lotr %>%
  mutate(author = "tolkien") %>%
  sdf_bind_rows(
    lww %>%
      mutate(author = "lewis")
  ) %>%
  filter(nchar(line) > 0) %>%
  glimpse()
```

We use the `Hive` `regexp_replace()` function to remove punctuation and numbers:

```{r}
all_text <- all_text %>%
  mutate(line = regexp_replace(line, "[_\"\'():;,.!?\\-]", " ")) %>%
  mutate(line = regexp_replace(line, "[0-9]", ""))
```

Next we tokenize the text and remove stop words using `ft_tokenizer()` and `ft_stop_words_remover()`:

```{r}
all_text <- all_text %>%
  ft_tokenizer(input_col = "line", output_col = "word_list") %>%
  ft_stop_words_remover(input_col = "word_list", output_col = "no_stop_word_list")

head(all_text)
```

Note that `ft_tokenizer()` splits on spaces, but we can also use the `ft_regex_tokenizer()` function to split on more complex patterns if we prefer. We see at this point that the `list`s in the `no_stop_word_list` column are typically smaller than the `list`s in the `word_list` column, confirming that stop words have been removed.

Now we use the `explode()` function in a mutate call to pull out individual words from the `no_stop_word_list` column, dropping this column afterwards. We store the results in an intermediate `all_text` `DataFrame` on `Spark`:

```{r}
all_text <- all_text %>%
  select(author, no_stop_word_list) %>%
  mutate(word = explode(no_stop_word_list)) %>%
  select(-no_stop_word_list) %>%
  filter(nchar(word) > 2) %>%
  compute("all_text") %>%
  glimpse()
```

Now let's get a count of the most frequent words used and the author who used them:

```{r}
word_count <- all_text %>%
  group_by(author, word) %>%
  tally() %>%
  arrange(desc(n)) 
  
word_count
```
In a slightly more complex examaple, let's find the words used by Tolkien and not used by Lewis

```{r}
tolkien_unique <- filter(word_count, author == "tolkien") %>%
  anti_join(filter(word_count, author == "lewis"), by = "word") %>%
  arrange(desc(n)) %>%
  compute("tolkien_unique")

tolkien_unique
```

Finally, we can adopt a `tidy` approach to sentiment analysis using the `tidytext` and `textdata` packages. We will use the `afinn` word scores developed by Finn Årup Nielsen for this. First we get the `afinn` scores and load them into `Spark`:

```{r}
library(tidytext)
library(textdata)

afinn_scores <- get_sentiments("afinn")
afinn_scores <- copy_to(sc, afinn_scores)

afinn_scores %>%
  glimpse()
```

Next we associate a score with each `word` in `all_text`, and compute the mean positivity per word for each `author` in the following way:

```{r}
all_text %>%
  left_join(afinn_scores, by = "word") %>%
  group_by(author) %>%
  summarise(positivity_per_word = sum(value) / n())
```

We see Lewis' language is rather neutral, whereas Tolkien's is slightly more positive.

# Benchmarking `Spark` execution times

Finally, we may often be interested in benchmarking execution times of different `sparklyr` operations. The `microbenchmark` package offers an excellent way to do this. We just wrap any code we wish to time inside a call to `microbenchmark()`. We can also tell the function how many times to run each piece of code (providing meaningful statistics on execution times), and we can also further control execution using the `control = ` argument. 

Here's an example timing sorting in `Spark` using the `arrange()` and `sdf_sort()` functions. We execute each piece of code 20 times, in random order, and 'warm up' by running each piece of code 10 times before we start gathering statistics:

```{r}
library(microbenchmark)
mb <- microbenchmark(
  arranged = prestige %>%
    arrange(income, women, education) %>%
    collect(),
  
  sorted = prestige %>%
    sdf_sort(c("income", "women", "education")) %>%
    collect(),
  
  times = 20,
  
  control = list(
    order = "random",
    warmup = 10
  )
)

print(mb)
```


# Saving data to Apache `Parquet` format 

Finally, you will often wish to write intermediate data sets to file. We recommend Apache `Parquet` for this purpose: it is a high performance columnar data storage format particularly well-suited for use with `Spark`. You can write `Parquet` with the `spark_write_parquet()` function: 

```{r}
spark_write_parquet(prestige, "prestige_data")
```

Once you've run this, you will see that `prestige_data` is created as a **directory**. Looking inside it, for a large data set you would typically find multiple `part-` files, as `Parquet` stores data in multiple files. In this case though the `Prestige` dataset is small, and we find just one `part-`.

You can read `Parquet` using the `spark_read_parquet()` function:

```{r}
prestige_again <- spark_read_parquet(sc, name = "prestige_again", path = "prestige_data") %>%
  glimpse()
```


